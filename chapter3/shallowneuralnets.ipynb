{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks, Chapter 3-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports math library\n",
    "import numpy as np\n",
    "# Imports plotting library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first construct the shallow neural network with one input, three hidden units, and one output described in section 3.1 of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "  # TODO write code to implement the ReLU and compute the activation at the\n",
    "  # hidden unit from the preactivation\n",
    "  # This should work on every element of the ndarray \"preactivation\" at once\n",
    "  # One way to do this is with the ndarray \"clip\" function\n",
    "  # https://numpy.org/doc/stable/reference/generated/numpy.ndarray.clip.html\n",
    "  activation = np.zeros_like(preactivation)\n",
    "  return activation     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an array of inputs\n",
    "z = np.arange(-5,5,0.1)\n",
    "RelU_z = ReLU(z)\n",
    "\n",
    "# Plot the ReLU function\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(z,RelU_z,'r-')\n",
    "ax.set_xlim([-5,5]);ax.set_ylim([-5,5])\n",
    "ax.set_xlabel('z'); ax.set_ylabel('ReLU[z]')\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a shallow neural network with, one input, one output, and three hidden units\n",
    "def shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
    "  # TODO Replace the lines below to compute the three initial lines\n",
    "  # (figure 3.3a-c) from the theta parameters.  These are the preactivations\n",
    "  pre_1 = np.zeros_like(x)\n",
    "  pre_2 = np.zeros_like(x)\n",
    "  pre_3 = np.zeros_like(x)\n",
    "\n",
    "  # Pass these through the ReLU function to compute the activations as in\n",
    "  # figure 3.3 d-f\n",
    "  act_1 = activation_fn(pre_1)\n",
    "  act_2 = activation_fn(pre_2)\n",
    "  act_3 = activation_fn(pre_3)\n",
    "\n",
    "  # TODO Replace the code below to weight the activations using phi1, phi2 and phi3\n",
    "  # To create the equivalent of figure 3.3 g-i\n",
    "  w_act_1 = np.zeros_like(x)\n",
    "  w_act_2 = np.zeros_like(x)\n",
    "  w_act_3 = np.zeros_like(x)\n",
    "\n",
    "  # TODO Replace the code below to combining the weighted activations and add\n",
    "  # phi_0 to create the output as in figure 3.3 j\n",
    "  y = np.zeros_like(x)\n",
    "\n",
    "  # Return everything we have calculated\n",
    "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the shallow neural network.  We'll assume input in is range [0,1] and output [-1,1]\n",
    "# If the plot_all flag is set to true, then we'll plot all the intermediate stages as in Figure 3.3\n",
    "def plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=False, x_data=None, y_data=None):\n",
    "\n",
    "  # Plot intermediate plots if flag set\n",
    "  if plot_all:\n",
    "    fig, ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(8.5, 8.5)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    ax[0,0].plot(x,pre_1,'r-'); ax[0,0].set_ylabel('Preactivation')\n",
    "    ax[0,1].plot(x,pre_2,'b-'); ax[0,1].set_ylabel('Preactivation')\n",
    "    ax[0,2].plot(x,pre_3,'g-'); ax[0,2].set_ylabel('Preactivation')\n",
    "    ax[1,0].plot(x,act_1,'r-'); ax[1,0].set_ylabel('Activation')\n",
    "    ax[1,1].plot(x,act_2,'b-'); ax[1,1].set_ylabel('Activation')\n",
    "    ax[1,2].plot(x,act_3,'g-'); ax[1,2].set_ylabel('Activation')\n",
    "    ax[2,0].plot(x,w_act_1,'r-'); ax[2,0].set_ylabel('Weighted Act')\n",
    "    ax[2,1].plot(x,w_act_2,'b-'); ax[2,1].set_ylabel('Weighted Act')\n",
    "    ax[2,2].plot(x,w_act_3,'g-'); ax[2,2].set_ylabel('Weighted Act')\n",
    "\n",
    "    for plot_y in range(3):\n",
    "      for plot_x in range(3):\n",
    "        ax[plot_y,plot_x].set_xlim([0,1]);ax[plot_x,plot_y].set_ylim([-1,1])\n",
    "        ax[plot_y,plot_x].set_aspect(0.5)\n",
    "      ax[2,plot_y].set_xlabel('Input, ');\n",
    "    plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(x,y)\n",
    "  ax.set_xlabel('Input, '); ax.set_ylabel('Output, ')\n",
    "  ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
    "  ax.set_aspect(0.5)\n",
    "  if x_data is not None:\n",
    "    ax.plot(x_data, y_data, 'mo')\n",
    "    for i in range(len(x_data)):\n",
    "      ax.plot(x_data[i], y_data[i],)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define some parameters and run the neural network\n",
    "theta_10 =  0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0  ; theta_21 = 2.0\n",
    "theta_30 = -0.5  ; theta_31 = 0.65\n",
    "phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "# And then plot it\n",
    "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
